{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from project import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing/Scraping Corpus\n",
    "\n",
    "We will use The Great Gatsby to train our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatsby = get_book('https://www.gutenberg.org/cache/epub/64317/pg64317.txt')\n",
    "# gatsby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(gatsby)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform Language Model\n",
    "\n",
    "A uniform language model is one in which each unique token is equally likely to appear in any position, unconditional of any other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cornets correctness stored wait eddies arms fingers against Wild cupboard series Fixed won lumps powdered Slenderly roller slipping Indies finding gloomily flannels always lounged stronger maybe association play rejected Blues warn conscious awkward faintest kitchen remarking detail vitality Works ties possible ghastly keys be bursts sail ferryboat violence Together temporarily damned brooding thoroughly positively ejaculated Lemme dyed Croirier Miss ineffable countenance Beast ecstatically motor Avenue convinced Know rapidly brothel transported telephoned band included extraordinarily spent bleak Both behind visitors hedge afterwards matter policeman saucer could several hilarity quieter cottages forever unjustly apartments Par contrast tears youth wrote headed ravages traffic'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unif = UniformLM(tokens)\n",
    "unif.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform model has an equal chance of chance of choosing every token. Thus this output makes no sense. The point of it is to demonstrate the simplest language model, and to get used to the coding needed for more complicated models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Language Model\n",
    "\n",
    "A unigram language model is one in which the probability assigned to a token is equal to the proportion of tokens in the corpus that are equal to said token. That is, the probability distribution associated with a unigram language model is just the empirical distribution of tokens in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s rushed morning less its on carriages \\x03 to too you ” Thomas his hour , that I , his could they of to ’ moment the ” I outside , rang of said \\x02 , yawned “ out and the \\x03 I end — - woman friendly sophisticated at . step his left ’ wherever at that . some are intended \\x02 \\x02 Tom . and indiscreet him water of a ’ on \\x02 , took around suddenly : had speech Gatsby , held blue see you understand ran I , that I out people just wondering ’ in'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram = UnigramLM(tokens)\n",
    "unigram.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this model doesn't make too much sense, but it is better than the uniform model since it takes into account the frequency of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Language Model\n",
    "\n",
    "Now we will build an N-Gram language model, in which the probability of a token appearing in a sentence does depend on the tokens that come before it.\n",
    "\n",
    "The N-Gram language model relies on the assumption that only nearby tokens matter. Specifically, it assumes that the probability that a token occurs depends only on the previous $N-1$ tokens, rather than all previous tokens. That is:\n",
    "\n",
    "$$P(w_n|w_1,\\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "When $N=3$, we have a \"trigram\" model. Such a model looks at the previous $N-1 = 2$ tokens when computing probabilities.\n",
    "\n",
    "Consider the tuple `('when', 'I', 'drink', 'Coke', 'I', 'smile')`, corresponding to the sentence `'when I drink Coke I smile'`. Under the trigram model, the probability of this sentence is computed as follows:\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I}) \\cdot P(\\text{Coke | I drink}) \\cdot P(\\text{I | drink Coke}) \\cdot P(\\text{smile | Coke I})$$\n",
    "\n",
    "The main issue is figuring out how to implement the hyperparameter N in this model. For example in the 3-gram model, we must also store a unigram model to determine the first token, and a bigram model to determine the second token (given the first token). How can we implement this without repeating code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = NGramLM(3, tokens)\n",
    "# llm.create_ngrams(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the N-Gram Language Model\n",
    "\n",
    "The N-Gram LM consists of probabilities of the form\n",
    "\n",
    "$$P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "which we estimate by  \n",
    "\n",
    "$$\\frac{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1}, w_n)}{C(w_{n-(N-1)}, w_{n-(N-2)}, \\ldots, w_{n-1})}$$\n",
    "\n",
    "for every N-Gram that occurs in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>n1gram</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-, -, -)</td>\n",
       "      <td>(-, -)</td>\n",
       "      <td>0.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(\u0003, \u0002, “)</td>\n",
       "      <td>(\u0003, \u0002)</td>\n",
       "      <td>0.587022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(”, \u0003, \u0002)</td>\n",
       "      <td>(”, \u0003)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(., \u0003, \u0002)</td>\n",
       "      <td>(., \u0003)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(., ”, \u0003)</td>\n",
       "      <td>(., ”)</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48866</th>\n",
       "      <td>(Jackson, Abrams, of)</td>\n",
       "      <td>(Jackson, Abrams)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48867</th>\n",
       "      <td>(Abrams, of, Georgia)</td>\n",
       "      <td>(Abrams, of)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48868</th>\n",
       "      <td>(of, Georgia, ,)</td>\n",
       "      <td>(of, Georgia)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48869</th>\n",
       "      <td>(Georgia, ,, and)</td>\n",
       "      <td>(Georgia, ,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48870</th>\n",
       "      <td>(past, ., \u0003)</td>\n",
       "      <td>(past, .)</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48871 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ngram             n1gram      prob\n",
       "0                  (-, -, -)             (-, -)  0.985915\n",
       "1                  (\u0003, \u0002, “)             (\u0003, \u0002)  0.587022\n",
       "2                  (”, \u0003, \u0002)             (”, \u0003)  1.000000\n",
       "3                  (., \u0003, \u0002)             (., \u0003)  1.000000\n",
       "4                  (., ”, \u0003)             (., ”)  0.900000\n",
       "...                      ...                ...       ...\n",
       "48866  (Jackson, Abrams, of)  (Jackson, Abrams)  1.000000\n",
       "48867  (Abrams, of, Georgia)       (Abrams, of)  1.000000\n",
       "48868       (of, Georgia, ,)      (of, Georgia)  1.000000\n",
       "48869      (Georgia, ,, and)       (Georgia, ,)  1.000000\n",
       "48870           (past, ., \u0003)          (past, .)  0.250000\n",
       "\n",
       "[48871 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.mdl # for 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>n1gram</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-, -)</td>\n",
       "      <td>(-,)</td>\n",
       "      <td>0.867123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(\u0003, \u0002)</td>\n",
       "      <td>(\u0003,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(\u0002, “)</td>\n",
       "      <td>(\u0002,)</td>\n",
       "      <td>0.586667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(”, \u0003)</td>\n",
       "      <td>(”,)</td>\n",
       "      <td>0.539202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(., \u0003)</td>\n",
       "      <td>(.,)</td>\n",
       "      <td>0.251771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29621</th>\n",
       "      <td>(wrote, down)</td>\n",
       "      <td>(wrote,)</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29622</th>\n",
       "      <td>(Once, I)</td>\n",
       "      <td>(Once,)</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29623</th>\n",
       "      <td>(crystal, glass)</td>\n",
       "      <td>(crystal,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29624</th>\n",
       "      <td>(there, crystal)</td>\n",
       "      <td>(there,)</td>\n",
       "      <td>0.005780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29625</th>\n",
       "      <td>(ceaselessly, into)</td>\n",
       "      <td>(ceaselessly,)</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29626 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ngram          n1gram      prob\n",
       "0                   (-, -)            (-,)  0.867123\n",
       "1                   (\u0003, \u0002)            (\u0003,)  1.000000\n",
       "2                   (\u0002, “)            (\u0002,)  0.586667\n",
       "3                   (”, \u0003)            (”,)  0.539202\n",
       "4                   (., \u0003)            (.,)  0.251771\n",
       "...                    ...             ...       ...\n",
       "29621        (wrote, down)        (wrote,)  0.333333\n",
       "29622            (Once, I)         (Once,)  0.125000\n",
       "29623     (crystal, glass)      (crystal,)  1.000000\n",
       "29624     (there, crystal)        (there,)  0.005780\n",
       "29625  (ceaselessly, into)  (ceaselessly,)  0.500000\n",
       "\n",
       "[29626 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.prev_mdl.mdl # for bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".            0.046488\n",
       ",            0.044437\n",
       "the          0.033287\n",
       "-            0.030638\n",
       "\u0002            0.024696\n",
       "               ...   \n",
       "deeply       0.000015\n",
       "flicked      0.000015\n",
       "protested    0.000015\n",
       "Either       0.000015\n",
       "borne        0.000015\n",
       "Name: proportion, Length: 6279, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.prev_mdl.prev_mdl.mdl # for unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x02 A dim background started to town ? ” \\x03 \\x02 “ I wouldn ’ t happy , and probably transported complete from some ruin overseas . She had it coming to tea . ” \\x03 \\x02 “ You wait here till Daisy goes to bed . Good night , old sport , ” said Tom intently . “ Mrs . Wilson ’ s going whether she wants to be looked at my front door opened nervously , and simultaneously there was one of the corners , as though she were balancing something on now that I had stayed so \\x03'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model makes a lot more sense than the previous ones. To improve the model we can increaes the size of the training corpus and increase N."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
